{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34006ee1-51e5-4a48-82de-021a71e1201a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# StatQuest: Word Embedding in PyTorch + Lightning!!!\n",
    "## Brought to you by...\n",
    "[<img src=\"./images/Brandmark_FullColor_Black.png\" alt=\"Lightning\" style=\"width: 400px;\">](https://www.pytorchlightning.ai/)\n",
    "\n",
    "Copyright 2023, Joshua Starmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0aedd-9400-4ce1-8885-d800f997cb6b",
   "metadata": {},
   "source": [
    "---- \n",
    "**NOTE:** This tutorial is from the StatQuest **[Word Embedding in PyTorch + Lightning](https://youtu.be/Qf06XDYXCXI)**.\n",
    "\n",
    "In this tutorial, we will use **[PyTorch](https://pytorch.org/) **+** [Lightning](https://www.lightning.ai/)** to create and optimize word embeddings using the incredibly simple network seen below and featured in the **StatQuest** **[Word Embedding and Word2Vec, Clearly Explained!!](https://youtu.be/viZrOnJclY0)** \n",
    "\n",
    "<img src=\"./images/word_embedding_network.png\" alt=\"A simple word embedding network\" style=\"width: 600px;\">\n",
    "\n",
    "In that **StatQuest**, this simple network created word embeddings that made two movie titles, **Troll 2** and **Gymkata**, cluster together because they were used in similar contexts.\n",
    "\n",
    "<img src=\"./images/trained_embedding_graph.png\" alt=\"A graph of the trained word embeddings\" style=\"width: 300px;\">\n",
    "\n",
    "<!-- The training data consists of two phrases, **Troll 2 is great!** and **Gymkata is great!**, where, for the sake of this demonstration, **Troll 2** is considered a single word. -->\n",
    "\n",
    "In this tutorial, you will...\n",
    "\n",
    "- **[Build and train a Word Embedding Unit from scratch](#build_entirely_by_hand)**\n",
    "\n",
    "- **[Build and train a Word Embedding Unit using `nn.Linear()`](#build_entirely_with_linear)**\n",
    "\n",
    "- **[Use `nn.Embedding()` to load and use pre-trained Word Embeddings](#embedding_lookup)**\n",
    "\n",
    "- **BONUS BAM: [Build and train a word Word Embedding Unit using `nn.Embedding()` and `nn.Linear()`](#build_with_embedding)**\n",
    "\n",
    "\n",
    "#### NOTE:\n",
    "This tutorial assumes that you already know the basics of coding in **Python** and are familiar with the theory behind **[Neural Networks](https://youtu.be/CqOfi41LfDw)**, **[Backpropagation](https://youtu.be/IN2XmBhILt4)**, and **[Word Embedding](https://youtu.be/viZrOnJclY0)**. If not, check out the **'Quests** by clicking on the links for each topic.\n",
    "\n",
    "#### ALSO NOTE:\n",
    "I strongly encourage you to play around with the code. Playing with the code is the best way to learn from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3669a0-284c-48f7-9c73-19812e2a8fac",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4feccd-8472-4bb2-bc56-ea41e527d714",
   "metadata": {},
   "source": [
    "# Import the modules that will do all the work\n",
    "\n",
    "You will need **Python 3.8** and have at least these versions for each of the following modules: \n",
    "- pytorch >= 1.10.1\n",
    "- lightning >= 1.8.0\n",
    "\n",
    "**If you installed **Python** with [Anaconda](https://www.anaconda.com/)...**\n",
    "\n",
    "...then you can check which versions of each package you have with the command: `conda list`. If, for example, your version of `matplotlib` is older than **3.3.4**, then the easiest thing to do is just update all of your Anaconda packages with the following command: `conda update --all`. However, if you only want to update `matplotlib`, then you can run this command: `conda install matplotlib=3.3.4`.\n",
    "\n",
    "**If you need to install **PyTorch**...**\n",
    "\n",
    "...then the easiest thing to do is follow the instructions on the [PyTorch website](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "**If you need to install **Lightning**...**\n",
    "\n",
    "...then the easiest thing to do is follow the instructions on the [Lightning AI website](https://lightning.ai/lightning-docs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2719c054-5e2e-47e5-8ca8-f5e3572944ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# %%capture prevents this cell from printing a ton of STDERR stuff to the screen\n",
    "\n",
    "## NOTE: Uncomment the next line to install stuff if you need to.\n",
    "##       Also, installing can take a few minutes...\n",
    "# !pip install lightning seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6b9647-24a8-4c13-9665-a036d9a8e121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch # torch will allow us to create tensors.\n",
    "import torch.nn as nn # torch.nn allows us to create a neural network and allows\n",
    "                      # us to access a lot of useful functions like:\n",
    "                      # nn.Linear, nn.Embedding, nn.CrossEntropyLoss() etc.\n",
    "\n",
    "from torch.optim import Adam # optim contains many optimizers. This time we're using Adam\n",
    "from torch.distributions.uniform import Uniform # So we can initialize our tensors with a uniform distribution\n",
    "from torch.utils.data import TensorDataset, DataLoader # these are needed for the training data\n",
    "\n",
    "import lightning as L # lightning has tons of cool tools that make neural networks easier\n",
    "\n",
    "import pandas as pd ## to create dataframes from graph input\n",
    "import matplotlib.pyplot as plt ## matplotlib allows us to draw graphs.\n",
    "import seaborn as sns ## seaborn makes it easier to draw nice-looking graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31140c14-1161-48ab-ac69-73859196a422",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f3bd1-2796-4f93-b13b-eaa2bdda3467",
   "metadata": {},
   "source": [
    "# Create the datasets that we will use for training the word embeddings\n",
    "\n",
    "Now let's create our training data from two phrases, **Troll 2 is great** and **Gymkata is great**, which gives us a simple 4 word, or token, vocabulary: **Troll 2**, **is**, **great**, **Gymkata**. Our training data consists of two parts: `inputs`, the inputs to the neural network, and `labels`, the expected outputs from the neural networks.\n",
    "\n",
    "The idea is to have each token in a phrase predict the token that follows. For example, using **one-hot-encoding** to represent each token, since **Troll 2** is the first token in our vocalbuary, we will encode the `input` for **Troll 2** with `[1., 0., 0., 0.]`. And since **Troll 2** predicts the second token, **is**, which is the second token in our vocabulary, we will encode the `label` for **Troll 2** with `[0., 1., 0., 0.]`. Likewise, we can encode the `inputs` and `labels` for **is**, **great** and **Gymkata**. **NOTE: Gymkata** predicts the second token, **is**, so it's label is `[0., 1., 0., 0.]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cb3602-b0d5-457c-a5da-97cd3d3f9efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## create the training data for the neural network.\n",
    "inputs = torch.tensor([[1., 0., 0., 0.], # one-hot-encoding for Troll 2...\n",
    "                       [0., 1., 0., 0.], # ...is\n",
    "                       [0., 0., 1., 0.], # ...great\n",
    "                       [0., 0., 0., 1.]]) # ...Gymkata\n",
    "\n",
    "labels = torch.tensor([[0., 1., 0., 0.], # \"Troll 2\" is followed by \"is\"\n",
    "                       [0., 0., 1., 0.], # \"is\" is followed by \"great\"\n",
    "                       [0., 0., 0., 1.], # \"great\" isn't followed by anything, but we'll pretend it was followed by \"Gymkata\"\n",
    "                       [0., 1., 0., 0.]]) # \"Gymkata\", just like \"Troll 2\", is followed by \"is\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9168948f-5e25-48ad-b775-ef04463d9a3a",
   "metadata": {},
   "source": [
    "Now that we have created the data that we want to train the embeddings with, we'll store it in a `DataLoader`. Since our dataset is so small, using a `DataLoader` is a little bit of an overkill, but it it's easy to do, and it will allow us to easily scale up to a much larger vocabulary when the time comes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79749df5-3328-4d37-8a85-3fa8df641549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = TensorDataset(inputs, labels) \n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df333b81-aab6-4fed-b593-33439f05d74d",
   "metadata": {},
   "source": [
    "----\n",
    "<a id=\"build_entirely_by_hand\"></a>\n",
    "# Build and train a Word Embedding Unit from scratch\n",
    "\n",
    "Now that we have the data and `DataLoader` all worked out, let's create the `class` that will create the word embeddings for each token in the vacublary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b990a-3d31-4437-a995-f926c8cd4d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingFromScratch(L.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        ## __init__() initializes the weights and sets everything up for training\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ## The first thing we do is set the seed for the random number generorator.\n",
    "        ## This ensures that when someone creates a model from this class, that model\n",
    "        ## will start off with the exact same random numbers as I started out with when\n",
    "        ## I created this demo. At least, I hope that is what happens!!! :)\n",
    "        L.seed_everything(seed=42)\n",
    "        \n",
    "        ###################\n",
    "        ##\n",
    "        ## Initialize the weights.\n",
    "        ##\n",
    "        ## NOTE: We're initializing the weights using values from a uniform distribtion\n",
    "        ##       that goes from -0.5 to 0.5 (this is notated as U(-0.5, 0.5). \n",
    "        ##       This is because of how nn.Linear() initializes weights -\n",
    "        ##       nn.Linear() uses U(-sqrt(k), sqrt(k)), where k=1/in_features.\n",
    "        ##       In our case, we have 4 inputs, so k=1/4 = 0.25. And the sqrt(0.25) = 0.5.\n",
    "        ##       Thus, nn.Linear() would use U(-0.5, 0.5) to initialize the weights, so \n",
    "        ##       that's what we'll do here as well.\n",
    "        ##\n",
    "        ###################\n",
    "        min_value = -0.5\n",
    "        max_value = 0.5\n",
    "        \n",
    "        ## Now we initialize the weights that feed 4 inputs (one for each unique word) \n",
    "        ##       into the 2 nodes in the hidden layer (top and bottom nodes)\n",
    "        ##\n",
    "        ## NOTE: Because we want words (or tokens) that are used in the same context to have similar\n",
    "        ##       weights, we are excluding bias terms from the connections from the inputs to the\n",
    "        ##       nodes in the hidden layer (alternatively, you could think that\n",
    "        ##       we set the bias terms to 0 and are not going to optimize them).\n",
    "        ##\n",
    "        ## ALSO NOTE: We're using nn.Parameter() here instead of torch.tensor() because we want\n",
    "        ##       to easily print out the parameters before and after training. Parameters are just\n",
    "        ##       tensors that are added to model's parameter list.\n",
    "        self.input1_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input1_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "\n",
    "        self.input2_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input2_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "\n",
    "        self.input3_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input3_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "\n",
    "        self.input4_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.input4_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        \n",
    "        ## Now we initialize the weights that come out of the hidden layer to the \"output\"\n",
    "        ## NOTE: Again, we are excluding bias terms. This time, we exclude them simply because\n",
    "        ##       we do not need them.\n",
    "        self.output1_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output1_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "\n",
    "        self.output2_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output2_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        \n",
    "        self.output3_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output3_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        \n",
    "        self.output4_w1 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "        self.output4_w2 = nn.Parameter(Uniform(min_value, max_value).sample())\n",
    "\n",
    "        ## For the loss function, we'll use CrossEntropyLoss, which we'll use in training_step()\n",
    "        ## NOTE: The nn.CrossEntropyLoss automatically applies softmax for us, so we don't need to import it.\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        \n",
    "    def forward(self, input): \n",
    "        ## forward() is where we do the math associated with running the inputs through the\n",
    "        ## network\n",
    "        \n",
    "        ## The input is delivered inside of a list, like this...\n",
    "        ##   [[1., 0., 0., 0.]]\n",
    "        ## ...and it's just easier if we remove the extra pair of brackets so we only have...\n",
    "        ##   [1., 0., 0., 0.]\n",
    "        ## ...so let's do it.\n",
    "        input = input[0]\n",
    "        \n",
    "        ## First, for the top node in the hidden layer, \n",
    "        ## we multiply each input by its weight, \n",
    "        ## and then calculate the sum of those products...\n",
    "        inputs_to_top_hidden = ((input[0] * self.input1_w1) + \n",
    "                                (input[1] * self.input2_w1) + \n",
    "                                (input[2] * self.input3_w1) + \n",
    "                                (input[3] * self.input4_w1))\n",
    "        \n",
    "        ## ...then, for the bottom node in the hidden layer,\n",
    "        ## we multiply each input by its weight, \n",
    "        ## and then calculate the sum of those products.\n",
    "        inputs_to_bottom_hidden = ((input[0] * self.input1_w2) +\n",
    "                                   (input[1] * self.input2_w2) +\n",
    "                                   (input[2] * self.input3_w2) +\n",
    "                                   (input[3] * self.input4_w2))\n",
    "        \n",
    "        ## Now, in theory, we could run inputs_to_top_hidden and inputs_to_bottom_hidden through \n",
    "        ## linear activation functions, but the outputs would be the exact same as in the inputs, \n",
    "        ## so we can just skip that step and instead compute the 4 output values from the 2 nodes in hidden layer\n",
    "        ## by summing the products of the hidden layer values and a pair of weights for each output.\n",
    "        output1 = ((inputs_to_top_hidden * self.output1_w1) + \n",
    "                   (inputs_to_bottom_hidden * self.output1_w2))\n",
    "        output2 = ((inputs_to_top_hidden * self.output2_w1) + \n",
    "                   (inputs_to_bottom_hidden * self.output2_w2))\n",
    "        output3 = ((inputs_to_top_hidden * self.output3_w1) + \n",
    "                   (inputs_to_bottom_hidden * self.output3_w2))\n",
    "        output4 = ((inputs_to_top_hidden * self.output4_w1) + \n",
    "                   (inputs_to_bottom_hidden * self.output4_w2))\n",
    "        \n",
    "        ## Now we need to concatenate the 4 output tensors so that we can run them through \n",
    "        ## the SoftMax function. However, because they are tensors (and have gradients attached to them), \n",
    "        ## we can't just combine them in a simple list like this...\n",
    "        # output_values = [output1, output2, output3, output4] ## THIS WILL NOT WORK\n",
    "        ## ...because that would strip off the gradients. \n",
    "        ## Instead, we use torch.stack(), which retains the gradients.\n",
    "        output_presoftmax = torch.stack([output1, output2, output3, output4])\n",
    "        ## NOTE: The the loss function we are using, nn.CrossEntropyLoss, automatically applies softmax for us, so we\n",
    "        ##       need to do that ourselves. If we want to actually use this network to predict the next word\n",
    "        ##       (instead of just using it for the Word Embedding values), then we'll need to apply the softmax() function\n",
    "        ##       ourselves (or just look to see what output value is largest).\n",
    "        \n",
    "        return(output_presoftmax)\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self): \n",
    "        # configure_optimizers() configures the optimizer we want to use for backpropagation.\n",
    "        \n",
    "        return Adam(self.parameters(), lr=0.1) # lr=0.1 sets the learning rate to 0.1\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx): \n",
    "        # training_step() takes a step of gradient descent.\n",
    "\n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i) # run input through the neural network\n",
    "        loss = self.loss(output_i, label_i[0]) ## loss = cross entropy\n",
    "                    \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ef5568-5826-4e07-b807-5764060f39ad",
   "metadata": {},
   "source": [
    "Now that we have created our new `class`, `WordEmbeddingFromScratch`, let's create a model and print out the initial parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819df074-8ece-4808-a2ec-1c4d258a2094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelFromScratch = WordEmbeddingFromScratch() # create the model...\n",
    "\n",
    "print(\"Before optimization, the parameters are...\")\n",
    "for name, param in modelFromScratch.named_parameters():\n",
    "    print(name, torch.round(param.data, decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85af72-311c-4fd4-94ce-caec261d0380",
   "metadata": {},
   "source": [
    "Notice how the weights for **input1** (`w1 = 0.3832` and `w2 = 0.4150`) and **input4** (`w1 = -0.2434` and `w2 = 0.2936`) are very different, even though they both represent movie titles (**Troll2** and **Gymkata**) that are used in the same context. We can visualize how similar, and different, the embeddings are for all four tokens by plotting them on a graph that has the **w1** values, the embedding values that go to the top node in the hidden layer, on the x-axis and the **w2** values, the embedding values that go to the bottom node in the hidden layer, on the y-axis. First, let's organize the data into a Pandas `DataFrame()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb225dd9-549d-4521-8f27-54900e20566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"w1\": [modelFromScratch.input1_w1.item(), ## item() pulls out the tensor value as a float\n",
    "           modelFromScratch.input2_w1.item(), \n",
    "           modelFromScratch.input3_w1.item(), \n",
    "           modelFromScratch.input4_w1.item()],\n",
    "    \"w2\": [modelFromScratch.input1_w2.item(), \n",
    "           modelFromScratch.input2_w2.item(), \n",
    "           modelFromScratch.input3_w2.item(), \n",
    "           modelFromScratch.input4_w2.item()],\n",
    "    \"token\": [\"Troll2\", \"is\", \"great\", \"Gymkata\"],\n",
    "    \"input\": [\"input1\", \"input2\", \"input3\", \"input4\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab0caa9-9108-4ced-92b2-a4a68ae4ebc2",
   "metadata": {},
   "source": [
    "Now let's use the dataframe we just created, `df`, and let's draw a scatter plot of the weights, `w1` and `w2`, for each token in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc9ea88-abcc-46d6-b257-a270d1ef5bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"w1\", y=\"w2\")\n",
    "\n",
    "## add the token that each dot represents to the graph\n",
    "# Troll 2\n",
    "plt.text(df.w1[0], df.w2[0], df.token[0], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold')\n",
    "\n",
    "# is\n",
    "plt.text(df.w1[1], df.w2[1], df.token[1], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold') \n",
    "\n",
    "# great\n",
    "plt.text(df.w1[2], df.w2[2], df.token[2], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold')\n",
    "\n",
    "# Gymkata\n",
    "plt.text(df.w1[3], df.w2[3], df.token[3], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1728efe-638f-4057-a581-ba1871e338c6",
   "metadata": {},
   "source": [
    "In the graph we can see that the weights for **Troll2** (representing **input1**) and **Gymkata** (representing **input4**) are no more similar to each other than the other inputs. However, by training this neural network, we hope that those weights will become similar. So lets create a `Trainer` and train the embedding network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5f1bc-fcef-4cac-b33e-1225ec246f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(max_epochs=100)\n",
    "trainer.fit(modelFromScratch, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c39ac0-9583-4326-9461-9977ed81e78c",
   "metadata": {},
   "source": [
    "Now, with the trained neural network, let's print out the values for each weight..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b1d6a9-b64e-49f7-a133-573f0da0249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After optimization, the parameters are...\")\n",
    "for name, param in modelFromScratch.named_parameters():\n",
    "    print(name, torch.round(param.data, decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9d6e38-3d85-4487-946c-e269ad2abe62",
   "metadata": {},
   "source": [
    "...and after **100** epochs, the weights for **input1** (`w1 = 2.0244` and `w2 = 1.9754`) are now relatively similar to the weights for **input2** (`w1 = 1.7264` and `w2 = 1.9912`). Just like before, we can illustrate how similar, and different, the embeddings are for all four tokens by plotting them on a graph that has the **w1** values on the x-axis and the **w2** values on the y-axis.\n",
    "\n",
    "First, let's create the pandas `DataFrame()`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d379cdb-4f2e-4d02-8f56-a2e2928b76c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"w1\": [modelFromScratch.input1_w1.item(), ## item() pulls out the tensor value as a float\n",
    "           modelFromScratch.input2_w1.item(), \n",
    "           modelFromScratch.input3_w1.item(), \n",
    "           modelFromScratch.input4_w1.item()],\n",
    "    \"w2\": [modelFromScratch.input1_w2.item(), \n",
    "           modelFromScratch.input2_w2.item(), \n",
    "           modelFromScratch.input3_w2.item(), \n",
    "           modelFromScratch.input4_w2.item()],\n",
    "    \"token\": [\"Troll2\", \"is\", \"great\", \"Gymkata\"],\n",
    "    \"input\": [\"input1\", \"input2\", \"input3\", \"input4\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57088407-e05e-4fa8-990e-1df8d63c1dc3",
   "metadata": {},
   "source": [
    "...and then draw the scatterplot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eddf08-b972-4530-866e-797ef0dad6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"w1\", y=\"w2\")\n",
    "\n",
    "## NOTE: For Troll2 and and Gymkata, we're adding offsets to where to print the tokens because otherwise\n",
    "## they will be so close to each other that they will overlap and be unreadable.\n",
    "\n",
    "## Troll 2\n",
    "plt.text(df.w1[0]-0.2, df.w2[0]+0.1, df.token[0], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold')\n",
    "\n",
    "## is\n",
    "plt.text(df.w1[1], df.w2[1], df.token[1], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold')\n",
    "\n",
    "## great\n",
    "plt.text(df.w1[2], df.w2[2], df.token[2],\n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold')\n",
    "\n",
    "## gymkata\n",
    "plt.text(df.w1[3]-0.3, df.w2[3]-0.3, df.token[3], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9fca6a-21a6-4de4-b9cf-87818bbe3ac7",
   "metadata": {},
   "source": [
    "...and after training the neural network, we see that the weights for **Troll2** and **Gymkata** are relatively similar compared to the weights for the other tokens. This is because both tokens have the same context.\n",
    "\n",
    "Lastly, we can verify that each token in the vacabulary correctly predicts the token that follows it by running the **one-hot-encoded** input values for each token through the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2445794-e8b6-4c2d-8b96-297544b21384",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's see what the model predicts\n",
    "\n",
    "## First, let's create a softmax object...\n",
    "softmax = nn.Softmax(dim=0) ## dim=0 applies softmax to rows, dim=1 applies softmax to columns\n",
    "\n",
    "## Now let's...\n",
    "\n",
    "## print the predictions for \"Troll2\"\n",
    "print(torch.round(softmax(modelFromScratch(torch.tensor([[1., 0., 0., 0.]]))), \n",
    "                  decimals=2)) \n",
    "\n",
    "## print the predictions for \"is\"\n",
    "print(torch.round(softmax(modelFromScratch(torch.tensor([[0., 1., 0., 0.]]))), \n",
    "                  decimals=2)) \n",
    "\n",
    "## print the predictions for \"great\"\n",
    "print(torch.round(softmax(modelFromScratch(torch.tensor([[0., 0., 1., 0.]]))), \n",
    "                  decimals=2)) \n",
    "\n",
    "## print the predictions for \"Gymkata\"\n",
    "print(torch.round(softmax(modelFromScratch(torch.tensor([[0., 0., 0., 1.]]))), \n",
    "                  decimals=2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf631d-fb57-4b67-b172-16facae4099f",
   "metadata": {},
   "source": [
    "And we see that all tokens correctly predict (give the highest probability to) the token that follows them. In this case, both **Troll2** and **Gymkata** both correctly predict the second token, **is**, with probability **1.0**.\n",
    "\n",
    "## BAM!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f30e61-b0dc-4e8f-9aa3-63ab0ba96759",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839e1b56-7043-4ab9-9a23-e2341b35528e",
   "metadata": {},
   "source": [
    "<a id=\"build_entirely_with_linear\"></a>\n",
    "# Build and train a Word Embedding Unit using `nn.Linear()`\n",
    "\n",
    "Now that we can make a Word Embedding network from scratch, let's simplify the code using `nn.Linear()`. `nn.Linear()` will make initialzing the tensors super easy and the math we do in the `forward()` step will also be much simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb7df2f-c0f7-4abf-93ea-f9a3dd956aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingWithLinear(L.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ## The first thing we do is set the seed for the random number generorator.\n",
    "        ## This ensures that when someone creates a model from this class, that model\n",
    "        ## will start off with the exact same random numbers as I started out with when\n",
    "        ## I created this demo. At least, I hope that is what happens!!! :)\n",
    "        L.seed_everything(seed=42)\n",
    "        \n",
    "        ## In order to initialize weights from the 4 inputs (one for each unique word) \n",
    "        ##       to the 2 nodes in the hidden layer (top and bottom nodes), we simply make\n",
    "        ##       one call to nn.Linear() where in_features specifies the number of\n",
    "        ##       inputs and out_features specifies the number of nodes we\n",
    "        ##       are connecting them to. Since we don't want to use bias terms,\n",
    "        ##       we set bias=False\n",
    "        self.input_to_hidden = nn.Linear(in_features=4, out_features=2, bias=False)\n",
    "        ## Now, in order to connect the 2 nodes in the hidden layer to the 4 outputs, we\n",
    "        ##       make one call to nn.Linear(), where in_features specifies the number of\n",
    "        ##       nodes in hidden layer and out_features specifies the number of output values we want.\n",
    "        ##       And again, we can set bias=False\n",
    "        self.hidden_to_output = nn.Linear(in_features=2, out_features=4, bias=False)\n",
    "        \n",
    "        ## We'll use CrossEntropyLoss in training_step()\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        \n",
    "    def forward(self, input): \n",
    "        \n",
    "        ## Unlike before, where we did all the math by hand, now we can\n",
    "        ## simply pass the input values to the weights we created with nn.Linear() \n",
    "        ## between the input and the hidden layer and save the result in \"hidden\"\n",
    "        ##\n",
    "        ## NOTE: Unlike before, we don't need to strip off the extra brackets from the\n",
    "        ##       input. the Linear ojbect knows what to do.\n",
    "        hidden = self.input_to_hidden(input)\n",
    "        \n",
    "        ## Then we pass \"hidden\" to the weights we created with nn.Linear() \n",
    "        ## between the hidden layer and the output.\n",
    "        output_values = self.hidden_to_output(hidden)\n",
    "                \n",
    "        return(output_values)\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self): \n",
    "        # this configures the optimizer we want to use for backpropagation.\n",
    "        \n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx): \n",
    "        # take a step during gradient descent.\n",
    "        \n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i) # run input through the neural network\n",
    "        loss = self.loss(output_i, label_i) ## loss = cross entropy\n",
    "                    \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb2afd6-c1f0-4805-8916-8fb3e92a65c8",
   "metadata": {},
   "source": [
    "Now that we have created our new `class`, `WordEmbeddingWithLinear`, let's create a model and print out the initial parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c00e9a7-3a31-4e22-8e78-f7bdb5b0e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLinear = WordEmbeddingWithLinear() \n",
    "\n",
    "print(\"Before optimization, the parameters are...\")\n",
    "for name, param in modelLinear.named_parameters():\n",
    "    print(name, torch.round(param.data, decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06f215c-dc33-4dfb-8b5f-1e3b4fb6d4bf",
   "metadata": {},
   "source": [
    "...and create a `DataFrame` with the embedding values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23465b4e-5332-431f-8cc2-9d468939a618",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    ## NOTE: Unlike before, when we called item() on each individual\n",
    "    ##       Weight, now that we are using nn.Linear, we access the\n",
    "    ##       Weights with \".weight\". We then have to remove the gradients\n",
    "    ##       associated with each Weight, so we also call .detach().\n",
    "    ##       Lastly, we then convert the tensor to a numpy array with\n",
    "    ##       numpy().\n",
    "    \"w1\": modelLinear.input_to_hidden.weight.detach()[0].numpy(), # [0] = Weights to top activation function\n",
    "    \"w2\": modelLinear.input_to_hidden.weight.detach()[1].numpy(), # [1] = Weights to bottom activation function\n",
    "    \"token\": [\"Troll2\", \"is\", \"great\", \"Gymkata\"],\n",
    "    \"input\": [\"input1\", \"input2\", \"input3\", \"input4\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87ae627-6ed2-4417-a81d-9efd8e4858fc",
   "metadata": {},
   "source": [
    "...and draw a scatter plot of the initial, unoptimized, embedding values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b359f-3a3f-4835-84ef-d45600badd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"w1\", y=\"w2\")\n",
    "\n",
    "## add the token each dot represents to the graph\n",
    "\n",
    "## Troll 2\n",
    "plt.text(df.w1[0], df.w2[0], df.token[0], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold') \n",
    "## is\n",
    "plt.text(df.w1[1], df.w2[1], df.token[1], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold')\n",
    "## great\n",
    "plt.text(df.w1[2], df.w2[2], df.token[2], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold')\n",
    "\n",
    "## Gymkata\n",
    "plt.text(df.w1[3], df.w2[3], df.token[3], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8409a0-453a-459a-8475-ecafbed5d910",
   "metadata": {},
   "source": [
    "...and we see that the embedding values for **Troll 2** and **Gymkata** are relatively different. So let's train the model and see if they become more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2862797-d87e-434f-9758-698cccb51d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(max_epochs=100)\n",
    "trainer.fit(modelLinear, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba39de-2c04-4f78-b619-c673c328beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After optimization, the parameters are...\")\n",
    "for name, param in modelLinear.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7cab77-be81-42a4-ad47-90cde6c8fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"w1\": modelLinear.input_to_hidden.weight.detach()[0].numpy(),\n",
    "    \"w2\": modelLinear.input_to_hidden.weight.detach()[1].numpy(),\n",
    "    \"token\": [\"Troll2\", \"is\", \"great\", \"Gymkata\"],\n",
    "    \"input\": [\"input1\", \"input2\", \"input3\", \"input4\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ec7e8-7a0e-4e3c-8e65-88bcf6e1c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"w1\", y=\"w2\")\n",
    "\n",
    "## add the token each dot represents to the graph\n",
    "## NOTE: For Troll2 and and Gymkata, we're adding offsets to where to print the tokens because otherwise\n",
    "## they will be so close to each other that they will overlap and be unreadable.\n",
    "\n",
    "# Troll 2\n",
    "plt.text(df.w1[0]-0.2, df.w2[0]-0.3, df.token[0], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold') \n",
    "\n",
    "# is\n",
    "plt.text(df.w1[1], df.w2[1], df.token[1], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold')\n",
    "\n",
    "# great\n",
    "plt.text(df.w1[2], df.w2[2], df.token[2], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold')\n",
    "\n",
    "# Gymkata\n",
    "plt.text(df.w1[3]-0.3, df.w2[3]+0.2, df.token[3], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ad130f-5e94-4615-9916-0601eda24970",
   "metadata": {},
   "source": [
    "And, after training the model, we see that the embedding values for **Troll 2** and **Gymkata** are more similar than before.\n",
    "\n",
    "Lastly, we can verify that each token in the vacabulary correctly predicts the token that follows it by running the **one-hot-encoded** input values for each token through the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135f619-f564-4270-b984-23bc189b3769",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's see what the model predicts\n",
    "softmax = nn.Softmax(dim=1) ## dim=0 applies softmax to rows, dim=1 applies softmax to columns\n",
    "\n",
    "print(torch.round(softmax(modelLinear(torch.tensor([[1., 0., 0., 0.]]))), decimals=2)) ## print the predictions for \"Troll2\"\n",
    "print(torch.round(softmax(modelLinear(torch.tensor([[0., 1., 0., 0.]]))), decimals=2)) ## print the predictions for \"is\"\n",
    "print(torch.round(softmax(modelLinear(torch.tensor([[0., 0., 1., 0.]]))), decimals=2)) ## print the predictions for \"great\"\n",
    "print(torch.round(softmax(modelLinear(torch.tensor([[0., 0., 0., 1.]]))), decimals=2)) ## print the predictions for \"Gymkata\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ecd691-f713-4266-8725-a2fbde8f7dc3",
   "metadata": {},
   "source": [
    "## Double BAM!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45b6d83-b601-4407-a0c3-3578f9061ec1",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b8a06-ab36-441c-8963-f0b1d95fdff0",
   "metadata": {},
   "source": [
    "<a id=\"embedding_lookup\"></a>\n",
    "# Use `nn.Embedding()` to load and use pre-trained Word Embeddings\n",
    "\n",
    "Now that we have created embeddings for each token in the vocabulary, we can store them in an `nn.Embedding()` object so that we can access them with the tokens, rather than the one-hot-encoded versions of the tokens. This makes them easily portable to other applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d18b771-8e60-4389-8ee6-6c0b98801dc4",
   "metadata": {},
   "source": [
    "First, let's just print out the embedding values that we created in modelLinear and that we want to add to an `nn.Embedding()` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dec6c7-2aa0-4adc-8096-cf25b7b35a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLinear.input_to_hidden.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d619c92a-865e-424a-8463-46f78d5318e8",
   "metadata": {},
   "source": [
    "Now let's create an `nn.Embedding()` object and add those Embedding Values to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a3942-0be2-4310-b05b-39de6fe379d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  NOTE: We have to transpose the original embedding values (from w1 and w2) for nn.Embedding()\n",
    "##        and we do this with adding a '.T' to modelLinear.input_to_hidden.weight.T\n",
    "word_embedings = nn.Embedding.from_pretrained(modelLinear.input_to_hidden.weight.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f1a48b-76b8-4f2e-be7c-938a9b8e7691",
   "metadata": {},
   "source": [
    "Now let's print out the weights to make sure they are what we expect them to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b646de6-0027-447c-9938-f69bcf53f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedings.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd40ef57-32ec-4078-bf78-241c93bfc646",
   "metadata": {},
   "source": [
    "Now we can access the embeddings for each token directly with an index between 0 and 3, like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568cee2b-e7fc-4874-8a2b-d2344a682db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedings(torch.tensor(0)) # retrieve the embedding values for Troll 2..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdeca04-4187-418e-9f4e-04575165765a",
   "metadata": {},
   "source": [
    "...or we can create a dictionary and access the embedding values with words or tokens instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b001059-a593-45e0-b4b4-13f47901863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {'Troll2': 0,\n",
    "         'is': 1,\n",
    "         'great': 2,\n",
    "         'Gymkata': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be858c-51cc-4845-9b60-57eec729939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedings(torch.tensor(vocab['Troll2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a2cde0-c21f-4e4f-836b-9a5799269cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedings(torch.tensor(vocab['Gymkata']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16db81db-6f65-48ef-b6f5-7cb6a1f16230",
   "metadata": {},
   "source": [
    "# TRIPLE BAM!!!\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6ee2b3-4ca1-42a1-9fae-93dcc706c1df",
   "metadata": {},
   "source": [
    "# **Bonus!** \n",
    "<a id=\"build_with_embedding\"></a>\n",
    "# Build and train a Word Embedding Unit using `nn.Embedding()` and `nn.Linear()`\n",
    "\n",
    "Although less commonly done, we can replace `nn.Linear()` from the inputs to the activation functions with `nn.Embedding()` and train the embeddings directly in the lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e6033-ef32-46c2-ab36-25f6c251b09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingWithEmbedding(L.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ## The first thing we do is set the seed for the random number generorator.\n",
    "        ## This ensures that when someone creates a model from this class, that model\n",
    "        ## will start off with the exact same random numbers as I started out with when\n",
    "        ## I created this demo. At least, I hope that is what happens!!! :)\n",
    "        L.seed_everything(seed=42)\n",
    "        \n",
    "        self.embed = nn.Embedding(4, 2) # 4 = number of words in the vocabulary, 2 = 2 numbers per embedding\n",
    "        self.hidden_to_output = nn.Linear(2, 4, bias=False)\n",
    "  \n",
    "        ## We'll use CrossEntropyLoss in training_step()\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        \n",
    "    def forward(self, input): \n",
    "          \n",
    "        hidden = self.embed(input[0])    \n",
    "        output_values = self.hidden_to_output(hidden)                \n",
    "        \n",
    "        return(output_values)\n",
    "       \n",
    "        \n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i[0]) # run input through the neural network\n",
    "        loss = self.loss(output_i, label_i[0]) ## self.loss = cross entropy\n",
    "                    \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a516b61c-6502-443d-9711-c79d8a0e7644",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the training data for the neural network.\n",
    "\n",
    "## NOTE: nn.Embedding() applies one-hot-encoding to the input for us, so\n",
    "## the data that we use for training will look different from before. \n",
    "inputsForEmbed = torch.tensor([[0], [1], [2], [3]]) ## NOTE: Troll2 = 0, is = 1, great = 2, Gymkata = 3\n",
    "labels = torch.tensor([[0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.], [0., 1., 0., 0.]])\n",
    "\n",
    "datasetForEmbed = TensorDataset(inputsForEmbed, labels) \n",
    "dataloaderForEmbed = DataLoader(datasetForEmbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a2df0-682d-4a4d-b8c4-cbc65cbf0288",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelEmbed = WordEmbeddingWithEmbedding() \n",
    "\n",
    "print(\"Before optimization, the parameters are...\")\n",
    "for name, param in modelEmbed.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b6d167-10a6-4aab-96d6-3bb120a913a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = modelEmbed.embed.weight.detach().numpy()\n",
    "w1 = [weights[0][0], weights[1][0], weights[2][0], weights[3][0]]\n",
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5572e2-6508-49f9-bcf8-3f644f7ddb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = [weights[0][1], weights[1][1], weights[2][1], weights[3][1]]\n",
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28abf23a-3704-4e0f-8398-63173788ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"w1\": w1,\n",
    "    \"w2\": w2,\n",
    "    \"token\": [\"Troll2\", \"is\", \"great\", \"Gymkata\"],\n",
    "    \"input\": [\"input1\", \"input2\", \"input3\", \"input4\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ca2b2-4258-4625-8aac-d88fb107d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"w1\", y=\"w2\")\n",
    "\n",
    "## add the token each dot represents to the graph\n",
    "## NOTE: For Troll2 and and Gymkata, we're adding offsets to where to print the tokens because otherwise\n",
    "## they will be so close to each other that they will overlap and be unreadable.\n",
    "plt.text(df.w1[0], df.w2[0], df.token[0], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold') # Troll 2\n",
    "plt.text(df.w1[1], df.w2[1], df.token[1], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold') # is\n",
    "plt.text(df.w1[2], df.w2[2], df.token[2], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold') # great\n",
    "plt.text(df.w1[3], df.w2[3], df.token[3], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold') # Gymkata\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be779d5-5fdf-4363-bf49-ae41cc51eb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(max_epochs=100)\n",
    "trainer.fit(modelEmbed, train_dataloaders=dataloaderForEmbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ea237-abc9-4bc0-9049-b1af073e2cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After optimization, the parameters are...\")\n",
    "for name, param in modelEmbed.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fe1035-b0e6-47f8-93bd-69b60a30e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = modelEmbed.embed.weight.detach().numpy()\n",
    "w1 = [weights[0][0], weights[1][0], weights[2][0], weights[3][0]]\n",
    "w2 = [weights[0][1], weights[1][1], weights[2][1], weights[3][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756b2c0b-16a6-40a1-b0a2-a177e0e1e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"w1\": w1,\n",
    "    \"w2\": w2,\n",
    "    \"token\": [\"Troll2\", \"is\", \"great\", \"Gymkata\"],\n",
    "    \"input\": [\"input1\", \"input2\", \"input3\", \"input4\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869acfd0-6bc0-4abb-bc80-b1ea8f1b8fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"w1\", y=\"w2\")\n",
    "\n",
    "## add the token each dot represents to the graph\n",
    "## NOTE: For Troll2 and and Gymkata, we're adding offsets to where to print the tokens because otherwise\n",
    "## they will be so close to each other that they will overlap and be unreadable.\n",
    "plt.text(df.w1[0]-0.2, df.w2[0]-0.3, df.token[0], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold') # Troll 2\n",
    "plt.text(df.w1[1], df.w2[1], df.token[1], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold') # is\n",
    "plt.text(df.w1[2], df.w2[2], df.token[2], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold') # great\n",
    "plt.text(df.w1[3]-0.3, df.w2[3]+0.2, df.token[3], \n",
    "         horizontalalignment='left', \n",
    "         size='medium', \n",
    "         color='black', \n",
    "         weight='semibold')# Gymkata\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86755105-b05e-49f7-8a35-88198218d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's see what the model predicts\n",
    "softmax = nn.Softmax(dim=0) ## dim=0 applies softmax to rows, dim=1 applies softmax to columns\n",
    "\n",
    "print(torch.round(softmax(modelEmbed(torch.tensor([0])).detach()), decimals=2)) ## print the predictions for \"Troll2\"\n",
    "print(torch.round(softmax(modelEmbed(torch.tensor([1])).detach()), decimals=2)) ## print the predictions for \"is\"\n",
    "print(torch.round(softmax(modelEmbed(torch.tensor([2])).detach()), decimals=2)) ## print the predictions for \"great\"\n",
    "print(torch.round(softmax(modelEmbed(torch.tensor([3])).detach()), decimals=2)) ## print the predictions for \"Gymkata\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671c48ff-8a5a-4e36-8e9f-4a3be54d04bc",
   "metadata": {},
   "source": [
    "# BONUS BAM!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
